{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7911a85a",
   "metadata": {},
   "source": [
    "# **Fitting Logistic Regression,SVM,Decision Tree Classifier Models on UniversalBank dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5b1a1",
   "metadata": {},
   "source": [
    "## **SetUp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85455d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1127e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f9fce",
   "metadata": {},
   "source": [
    "## **Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e35ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "UB=pd.read_csv(\"./Downloads/UniversalBank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "UB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d1afcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>45.338400</td>\n",
       "      <td>20.104600</td>\n",
       "      <td>73.774200</td>\n",
       "      <td>93152.503000</td>\n",
       "      <td>2.396400</td>\n",
       "      <td>1.937938</td>\n",
       "      <td>1.881000</td>\n",
       "      <td>56.498800</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.06040</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1443.520003</td>\n",
       "      <td>11.463166</td>\n",
       "      <td>11.467954</td>\n",
       "      <td>46.033729</td>\n",
       "      <td>2121.852197</td>\n",
       "      <td>1.147663</td>\n",
       "      <td>1.747659</td>\n",
       "      <td>0.839869</td>\n",
       "      <td>101.713802</td>\n",
       "      <td>0.294621</td>\n",
       "      <td>0.305809</td>\n",
       "      <td>0.23825</td>\n",
       "      <td>0.490589</td>\n",
       "      <td>0.455637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9307.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1250.750000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>91911.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>93437.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>94608.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>96651.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID          Age   Experience       Income      ZIP Code  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000   5000.000000   \n",
       "mean   2500.500000    45.338400    20.104600    73.774200  93152.503000   \n",
       "std    1443.520003    11.463166    11.467954    46.033729   2121.852197   \n",
       "min       1.000000    23.000000    -3.000000     8.000000   9307.000000   \n",
       "25%    1250.750000    35.000000    10.000000    39.000000  91911.000000   \n",
       "50%    2500.500000    45.000000    20.000000    64.000000  93437.000000   \n",
       "75%    3750.250000    55.000000    30.000000    98.000000  94608.000000   \n",
       "max    5000.000000    67.000000    43.000000   224.000000  96651.000000   \n",
       "\n",
       "            Family        CCAvg    Education     Mortgage  Personal Loan  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000    5000.000000   \n",
       "mean      2.396400     1.937938     1.881000    56.498800       0.096000   \n",
       "std       1.147663     1.747659     0.839869   101.713802       0.294621   \n",
       "min       1.000000     0.000000     1.000000     0.000000       0.000000   \n",
       "25%       1.000000     0.700000     1.000000     0.000000       0.000000   \n",
       "50%       2.000000     1.500000     2.000000     0.000000       0.000000   \n",
       "75%       3.000000     2.500000     3.000000   101.000000       0.000000   \n",
       "max       4.000000    10.000000     3.000000   635.000000       1.000000   \n",
       "\n",
       "       Securities Account  CD Account       Online   CreditCard  \n",
       "count         5000.000000  5000.00000  5000.000000  5000.000000  \n",
       "mean             0.104400     0.06040     0.596800     0.294000  \n",
       "std              0.305809     0.23825     0.490589     0.455637  \n",
       "min              0.000000     0.00000     0.000000     0.000000  \n",
       "25%              0.000000     0.00000     0.000000     0.000000  \n",
       "50%              0.000000     0.00000     1.000000     0.000000  \n",
       "75%              0.000000     0.00000     1.000000     1.000000  \n",
       "max              1.000000     1.00000     1.000000     1.000000  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b996df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if there are any na's\n",
    "UB.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bf6194b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check if there are any categorical variables\n",
    "category_var_list = list(UB.select_dtypes(include='object').columns)\n",
    "category_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "52673ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4698\n",
       "1     302\n",
       "Name: CD Account, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking counts of data in target variable\n",
    "UB['CD Account'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c31fa",
   "metadata": {},
   "source": [
    "By looking at the counts above, we can say that dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79961884",
   "metadata": {},
   "source": [
    "## **To Address data imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "14ad9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls0 = UB[UB['CD Account']==0]\n",
    "cls1 = UB[UB['CD Account']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "57f44194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling the minority class to make the dataset balanced\n",
    "from sklearn.utils import resample\n",
    "UB_minority_resampled = resample(cls1, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=4698,    \n",
    "                                 random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f5597379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4698, 14) (4698, 14)\n"
     ]
    }
   ],
   "source": [
    "print(cls0.shape,UB_minority_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6458d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat both to get the final dataset\n",
    "UB_df=pd.concat([cls0,UB_minority_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2cabcb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050048</td>\n",
       "      <td>-0.052607</td>\n",
       "      <td>-0.051682</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>-0.035039</td>\n",
       "      <td>-0.032528</td>\n",
       "      <td>0.015015</td>\n",
       "      <td>-0.007804</td>\n",
       "      <td>-0.076171</td>\n",
       "      <td>-0.005900</td>\n",
       "      <td>-0.013241</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.065827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.050048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994290</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>-0.053170</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>-0.030699</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.016674</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.008568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experience</th>\n",
       "      <td>-0.052607</td>\n",
       "      <td>0.994290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014554</td>\n",
       "      <td>-0.054266</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>-0.024488</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.018358</td>\n",
       "      <td>0.010187</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.011596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>-0.051682</td>\n",
       "      <td>-0.024570</td>\n",
       "      <td>-0.014554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>-0.067693</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>0.261282</td>\n",
       "      <td>0.684867</td>\n",
       "      <td>-0.015684</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.113227</td>\n",
       "      <td>0.096495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP Code</th>\n",
       "      <td>0.022977</td>\n",
       "      <td>-0.053170</td>\n",
       "      <td>-0.054266</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>-0.040839</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.054157</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>0.041503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family</th>\n",
       "      <td>-0.035039</td>\n",
       "      <td>-0.010719</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.067693</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.037446</td>\n",
       "      <td>0.097435</td>\n",
       "      <td>-0.039309</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.031432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCAvg</th>\n",
       "      <td>-0.032528</td>\n",
       "      <td>-0.030699</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018659</td>\n",
       "      <td>0.163664</td>\n",
       "      <td>0.482904</td>\n",
       "      <td>-0.003554</td>\n",
       "      <td>0.255327</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>0.053998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0.015015</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.024488</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>-0.040839</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>-0.018659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062692</td>\n",
       "      <td>0.185192</td>\n",
       "      <td>-0.046437</td>\n",
       "      <td>0.037454</td>\n",
       "      <td>-0.001167</td>\n",
       "      <td>0.014995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mortgage</th>\n",
       "      <td>-0.007804</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.261282</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>0.037446</td>\n",
       "      <td>0.163664</td>\n",
       "      <td>0.062692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228812</td>\n",
       "      <td>-0.022352</td>\n",
       "      <td>0.153617</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>0.065788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personal Loan</th>\n",
       "      <td>-0.076171</td>\n",
       "      <td>0.016674</td>\n",
       "      <td>0.018358</td>\n",
       "      <td>0.684867</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.097435</td>\n",
       "      <td>0.482904</td>\n",
       "      <td>0.185192</td>\n",
       "      <td>0.228812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044699</td>\n",
       "      <td>0.449772</td>\n",
       "      <td>0.127025</td>\n",
       "      <td>0.086281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Securities Account</th>\n",
       "      <td>-0.005900</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.010187</td>\n",
       "      <td>-0.015684</td>\n",
       "      <td>0.054157</td>\n",
       "      <td>-0.039309</td>\n",
       "      <td>-0.003554</td>\n",
       "      <td>-0.046437</td>\n",
       "      <td>-0.022352</td>\n",
       "      <td>0.044699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458058</td>\n",
       "      <td>0.125697</td>\n",
       "      <td>0.083761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD Account</th>\n",
       "      <td>-0.013241</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.325348</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.255327</td>\n",
       "      <td>0.037454</td>\n",
       "      <td>0.153617</td>\n",
       "      <td>0.449772</td>\n",
       "      <td>0.458058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416232</td>\n",
       "      <td>0.536487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online</th>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.016381</td>\n",
       "      <td>0.113227</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>-0.001167</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>0.127025</td>\n",
       "      <td>0.125697</td>\n",
       "      <td>0.416232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditCard</th>\n",
       "      <td>0.065827</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.096495</td>\n",
       "      <td>0.041503</td>\n",
       "      <td>0.031432</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.014995</td>\n",
       "      <td>0.065788</td>\n",
       "      <td>0.086281</td>\n",
       "      <td>0.083761</td>\n",
       "      <td>0.536487</td>\n",
       "      <td>0.192806</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          ID       Age  Experience    Income  ZIP Code  \\\n",
       "ID                  1.000000 -0.050048   -0.052607 -0.051682  0.022977   \n",
       "Age                -0.050048  1.000000    0.994290 -0.024570 -0.053170   \n",
       "Experience         -0.052607  0.994290    1.000000 -0.014554 -0.054266   \n",
       "Income             -0.051682 -0.024570   -0.014554  1.000000 -0.001442   \n",
       "ZIP Code            0.022977 -0.053170   -0.054266 -0.001442  1.000000   \n",
       "Family             -0.035039 -0.010719   -0.010217 -0.067693  0.001937   \n",
       "CCAvg              -0.032528 -0.030699   -0.030962  0.654625  0.037238   \n",
       "Education           0.015015  0.003678   -0.024488 -0.016276 -0.040839   \n",
       "Mortgage           -0.007804  0.008136    0.008253  0.261282  0.014983   \n",
       "Personal Loan      -0.076171  0.016674    0.018358  0.684867 -0.007086   \n",
       "Securities Account -0.005900  0.009411    0.010187 -0.015684  0.054157   \n",
       "CD Account         -0.013241  0.012238    0.016343  0.325348  0.048307   \n",
       "Online             -0.012500  0.013203    0.016381  0.113227  0.018528   \n",
       "CreditCard          0.065827  0.008568    0.011596  0.096495  0.041503   \n",
       "\n",
       "                      Family     CCAvg  Education  Mortgage  Personal Loan  \\\n",
       "ID                 -0.035039 -0.032528   0.015015 -0.007804      -0.076171   \n",
       "Age                -0.010719 -0.030699   0.003678  0.008136       0.016674   \n",
       "Experience         -0.010217 -0.030962  -0.024488  0.008253       0.018358   \n",
       "Income             -0.067693  0.654625  -0.016276  0.261282       0.684867   \n",
       "ZIP Code            0.001937  0.037238  -0.040839  0.014983      -0.007086   \n",
       "Family              1.000000 -0.073415   0.018735  0.037446       0.097435   \n",
       "CCAvg              -0.073415  1.000000  -0.018659  0.163664       0.482904   \n",
       "Education           0.018735 -0.018659   1.000000  0.062692       0.185192   \n",
       "Mortgage            0.037446  0.163664   0.062692  1.000000       0.228812   \n",
       "Personal Loan       0.097435  0.482904   0.185192  0.228812       1.000000   \n",
       "Securities Account -0.039309 -0.003554  -0.046437 -0.022352       0.044699   \n",
       "CD Account          0.030974  0.255327   0.037454  0.153617       0.449772   \n",
       "Online              0.038809  0.068332  -0.001167  0.059405       0.127025   \n",
       "CreditCard          0.031432  0.053998   0.014995  0.065788       0.086281   \n",
       "\n",
       "                    Securities Account  CD Account    Online  CreditCard  \n",
       "ID                           -0.005900   -0.013241 -0.012500    0.065827  \n",
       "Age                           0.009411    0.012238  0.013203    0.008568  \n",
       "Experience                    0.010187    0.016343  0.016381    0.011596  \n",
       "Income                       -0.015684    0.325348  0.113227    0.096495  \n",
       "ZIP Code                      0.054157    0.048307  0.018528    0.041503  \n",
       "Family                       -0.039309    0.030974  0.038809    0.031432  \n",
       "CCAvg                        -0.003554    0.255327  0.068332    0.053998  \n",
       "Education                    -0.046437    0.037454 -0.001167    0.014995  \n",
       "Mortgage                     -0.022352    0.153617  0.059405    0.065788  \n",
       "Personal Loan                 0.044699    0.449772  0.127025    0.086281  \n",
       "Securities Account            1.000000    0.458058  0.125697    0.083761  \n",
       "CD Account                    0.458058    1.000000  0.416232    0.536487  \n",
       "Online                        0.125697    0.416232  1.000000    0.192806  \n",
       "CreditCard                    0.083761    0.536487  0.192806    1.000000  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UB_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c3b1dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the columns that are least significant \n",
    "UB_df.drop(['ID', 'ZIP Code'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c5585",
   "metadata": {},
   "source": [
    "## **Split Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5cf0965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(UB_df, test_size=0.3)\n",
    "target='CD Account'\n",
    "predictors=list(UB_df.columns)\n",
    "predictors.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7f12e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the numerical columns as svm is scale sensitive\n",
    "scaler = preprocessing.StandardScaler()\n",
    "cols_to_stdize = [ 'Age', 'Experience', \n",
    "                   'Income', 'Family', 'CCAvg', \n",
    "                   'Education', 'Mortgage']                \n",
    "               \n",
    "# Transform the predictors of training and validation sets\n",
    "train_df[cols_to_stdize] = scaler.fit_transform(train_df[cols_to_stdize]) # train_predictors is not a numpy array\n",
    "\n",
    "\n",
    "test_df[cols_to_stdize] = scaler.transform(test_df[cols_to_stdize]) # validation_target is now a series object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8fc80110",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train_df[predictors]\n",
    "train_y = train_df[target] \n",
    "test_X = test_df[predictors]\n",
    "test_y = test_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0996ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2464f3",
   "metadata": {},
   "source": [
    "## **Fitting Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "75ea014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression(penalty='none', max_iter=900)\n",
    "_ = log_reg_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = log_reg_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"default logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "abc454e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liblinear Solver\n",
    "log_reg_liblin_model = LogisticRegression(solver='liblinear').fit(train_X, np.ravel(train_y))\n",
    "model_preds = log_reg_liblin_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"liblinear logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0fae4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 Regularization\n",
    "log_reg_L2_model = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "_ = log_reg_L2_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = log_reg_L2_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"L2 logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3cf0ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 Regularization\n",
    "log_reg_L1_model = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "_ = log_reg_L1_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = log_reg_L1_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"L1 logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "37a32f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Regularization\n",
    "log_reg_elastic_model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, max_iter=1000)\n",
    "_ = log_reg_elastic_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = log_reg_elastic_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Elastic logistic\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d2300ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 974, 'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "#Using Random search on Logistic Regression\n",
    "score_measure=\"recall\"\n",
    "kfolds=5\n",
    "param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C' : np.logspace(-4, 4, 20),\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter' : np.arange(100,1000)\n",
    "    }\n",
    "                      \n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e2506dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, rand_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression Random Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "03185046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'C': 0.0001, 'max_iter': 976, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "#Conducting Grid search around the values from Random search\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'penalty' : ['l1'],\n",
    "    'C' : np.arange(0.0001,0.1),\n",
    "    'solver' : ['saga'],\n",
    "    'max_iter' : np.arange(974,1000)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = lr, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d47040f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression Grid Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea5a36",
   "metadata": {},
   "source": [
    "## **Fitting Support Vector Machine Model on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "18f9d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM using linear kernel\n",
    "svm_lin_model = SVC(kernel=\"linear\",probability=True)\n",
    "_ = svm_lin_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = svm_lin_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"linear svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9fca95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM using RBF Kernel\n",
    "svm_rbf_model = SVC(kernel=\"rbf\", C=10, gamma='scale')\n",
    "_ = svm_rbf_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = svm_rbf_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"rbf svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a2103973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM using poly kernel\n",
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=1,probability=True)\n",
    "_ = svm_poly_model.fit(train_X, np.ravel(train_y))\n",
    "model_preds = svm_poly_model.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"poly svm\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4c8bb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'kernel': 'linear', 'gamma': 0.1, 'degree': 11, 'C': 28}\n"
     ]
    }
   ],
   "source": [
    "#Using Random search on SVM\n",
    "score_measure=\"recall\"\n",
    "kfolds=5\n",
    "param_grid = {'C': np.arange(1,50),\n",
    "              'degree': np.arange(1,25),\n",
    "              'gamma':np.arange(0.1,1),\n",
    "              'kernel': ['linear','rbf','poly']\n",
    "              }        \n",
    "svm = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "56474f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, rand_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM Random Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8f6c80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'C': 25, 'degree': 10, 'gamma': 0.09, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "#Conducting Grid search around the values from Random search\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(25,40),\n",
    "              'degree': np.arange(10,20),\n",
    "              'gamma':np.arange(0.09,0.5),\n",
    "              'kernel': ['linear']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bb4f120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, rand_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM Grid Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d157b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decisiontree Classifier\n",
    "Dt=DecisionTreeClassifier(max_depth=10)\n",
    "Dt=Dt.fit(train_X,np.ravel(train_y))\n",
    "model_preds=Dt.predict(test_X)\n",
    "c_matrix = confusion_matrix(test_y, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree Classifier\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f291c78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'min_samples_split': 19, 'min_samples_leaf': 30, 'min_impurity_decrease': 0.0071, 'max_leaf_nodes': 150, 'max_depth': 12, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#Using Random search on Decision Tree Classifier\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,40),  \n",
    "    'min_samples_leaf': np.arange(1,40),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "\n",
    "rand_search = RandomizedSearchCV(estimator = Dt, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "21e5820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, rand_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree Random Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "55f7b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80000 candidates, totalling 400000 fits\n",
      "The best recall score is 1.0\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': 140, 'min_impurity_decrease': 0.007, 'min_samples_leaf': 25, 'min_samples_split': 15}\n"
     ]
    }
   ],
   "source": [
    "#Conducting Grid search around the values from Random search\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(15,25),  \n",
    "    'min_samples_leaf': np.arange(25,35),\n",
    "    'min_impurity_decrease': np.arange(0.0060, 0.0080, 0.001),\n",
    "    'max_leaf_nodes': np.arange(140,180), \n",
    "    'max_depth': np.arange(10,20), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = Dt, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X, train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6dc79f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree Grid Search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fafdd297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear svm</td>\n",
       "      <td>0.876907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Random Search</td>\n",
       "      <td>0.876907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM Grid Search</td>\n",
       "      <td>0.876907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree Random Search</td>\n",
       "      <td>0.876907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree Grid Search</td>\n",
       "      <td>0.876907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.938985</td>\n",
       "      <td>0.993294</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf svm</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.992548</td>\n",
       "      <td>0.948043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poly svm</td>\n",
       "      <td>0.926215</td>\n",
       "      <td>0.980626</td>\n",
       "      <td>0.926761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1 logistic</td>\n",
       "      <td>0.878680</td>\n",
       "      <td>0.962742</td>\n",
       "      <td>0.883117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default logistic</td>\n",
       "      <td>0.879745</td>\n",
       "      <td>0.958271</td>\n",
       "      <td>0.883545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liblinear logistic</td>\n",
       "      <td>0.875842</td>\n",
       "      <td>0.958271</td>\n",
       "      <td>0.880219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elastic logistic</td>\n",
       "      <td>0.877261</td>\n",
       "      <td>0.958271</td>\n",
       "      <td>0.881426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L2 logistic</td>\n",
       "      <td>0.877261</td>\n",
       "      <td>0.956036</td>\n",
       "      <td>0.881181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Random Search</td>\n",
       "      <td>0.523945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Grid Search</td>\n",
       "      <td>0.523945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               model  Accuracy    Recall        F1\n",
       "0                         linear svm  0.876907  1.000000  0.885516\n",
       "0                  SVM Random Search  0.876907  1.000000  0.885516\n",
       "0                    SVM Grid Search  0.876907  1.000000  0.885516\n",
       "0        Decision Tree Random Search  0.876907  1.000000  0.885516\n",
       "0          Decision Tree Grid Search  0.876907  1.000000  0.885516\n",
       "0           Decision Tree Classifier  0.938985  0.993294  0.939394\n",
       "0                            rbf svm  0.948209  0.992548  0.948043\n",
       "0                           poly svm  0.926215  0.980626  0.926761\n",
       "0                        L1 logistic  0.878680  0.962742  0.883117\n",
       "0                   default logistic  0.879745  0.958271  0.883545\n",
       "0                 liblinear logistic  0.875842  0.958271  0.880219\n",
       "0                   Elastic logistic  0.877261  0.958271  0.881426\n",
       "0                        L2 logistic  0.877261  0.956036  0.881181\n",
       "0  Logistic Regression Random Search  0.523945  0.000000  0.000000\n",
       "0    Logistic Regression Grid Search  0.523945  0.000000  0.000000"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.loc[:,[\"model\",\"Accuracy\",\"Recall\",\"F1\"]].sort_values(by=['Recall'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0133971",
   "metadata": {},
   "source": [
    "Looking at the above results, the top 5 models have the same Recall value. In this case, the metric we have to consider is Recall because it's important to not have False Positives(Customers who doesn't have CD Account predicted as they have a CD Account) predicted by a model.So the best models on this dataset are Linear SVM, SVM using Random search and Grid Search, Decision Tree using Random Search and Grid Search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
